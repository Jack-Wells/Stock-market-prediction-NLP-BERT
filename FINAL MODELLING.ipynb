{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\j\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\j\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\j\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Dropout, BatchNormalization\n",
    " \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "File = open('stocks_name.txt','r')\n",
    "\n",
    "Name = []\n",
    "Codes = []\n",
    "Type = []\n",
    "Add = []\n",
    "\n",
    "for line in File:\n",
    "    line = re.sub('\\n', '', line)\n",
    "    line = line.split('\\t')\n",
    "    Name.append(re.sub(' ', ' ',line[0].lower())+' ')\n",
    "    Type.append(line[2])\n",
    "    Add.append(line[1].lower()+ ' ')\n",
    "    if line[1][2:4] == '.A':\n",
    "        Codes.append('BT-A.L')\n",
    "    elif line[1][2] == '.':\n",
    "        Codes.append(line[1] + 'L')\n",
    "    else:\n",
    "        Codes.append(line[1] + '.L')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "def window(stock, brokers, size, delay):\n",
    "    \n",
    "    windows = []\n",
    "    binary_pred = []\n",
    "    pred = []\n",
    "        \n",
    "    \n",
    "    for i in range(len(stock) - delay - size):\n",
    "        \n",
    "        scaler.fit(stock[i: i + size])\n",
    "        \n",
    "        win = []\n",
    "        for j in range(size):\n",
    "            # comment top line to only include brokers opinions\n",
    "            #win.append(np.concatenate(scaler.transform(stock[i+j:i+j+1])))\n",
    "            win.append(brokers[i+j])\n",
    "            \n",
    "        windows.append(np.concatenate(win))\n",
    "        \n",
    "        pred.append(scaler.transform(stock[i+size:i + size + delay])[-1][-3] - scaler.transform(stock[i:i + size])[-1][-3]) # The actual value scaled appropriatly\n",
    "        \n",
    "        #Binary pred catagory\n",
    "        if scaler.transform(stock[i+size:i + size + delay])[-1][-3] < scaler.transform(stock[i:i + size])[-1][-3]:\n",
    "            binary_pred.append(1)\n",
    "        else:\n",
    "            binary_pred.append(0)\n",
    "    \n",
    "        \n",
    "    return windows, binary_pred, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Svm = svm.SVC(C=1)\n",
    "mod = XGBClassifier()\n",
    "\n",
    "SVM = []\n",
    "LOG = []\n",
    "XGB = []\n",
    "LSTm = []\n",
    "\n",
    "fSVM = []\n",
    "fLOG = []\n",
    "fXGB = []\n",
    "fLSTm = []\n",
    "\n",
    "SPLIT = []\n",
    "\n",
    "#fix codes\n",
    "#fix missing\n",
    "for code in Codes[1:]:\n",
    "\n",
    "    DF = pd.read_csv('all_data/' + code + '.csv')\n",
    "\n",
    "    days = 10\n",
    "\n",
    "    DF['BERT combined'] = DF['Bert'][::1].rolling(days, min_periods=0).sum()[::-1]\n",
    "\n",
    "    DF = DF.drop(['Bert','neg', 'neu','pos'], axis=1)\n",
    "    \n",
    "    DF = DF.dropna()\n",
    "    \n",
    "    Brokers = DF[['Buy1', 'Outperform1', 'Hold1', 'Underperform1', 'Sell1', 'BERT combined']].values\n",
    "\n",
    "    Stock = DF[['Open', 'High', 'Low', 'Close', 'Adj Close','Volume']].values\n",
    "    \n",
    "    \n",
    "    windows, binary_pred, pred = window(Stock, Brokers, 10, 10)\n",
    "    \n",
    "    \n",
    "    X_train = windows[:1000]\n",
    "    X_test = windows[1000:]\n",
    "    y_train = binary_pred[:1000]\n",
    "    y_test = binary_pred[1000:]\n",
    "    \n",
    "    if len(y_test) > 0:\n",
    "    \n",
    "        split = 0\n",
    "        for i in y_test:\n",
    "            if i == 0:\n",
    "                split+=1\n",
    "        SPLIT.append(split/len(y_test))\n",
    "\n",
    "        print('Split : ', split/len(y_test))\n",
    "\n",
    "        Svm.fit(X_train, y_train)\n",
    "        y_pred = Svm.predict(X_test)\n",
    "        SVm = accuracy_score(y_test, y_pred) \n",
    "        fSVm = f1_score(y_test, y_pred, average='macro')\n",
    "        print('SVM : ', SVm, fSVm)\n",
    "\n",
    "        clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        log = accuracy_score(y_test, y_pred) \n",
    "        flog = f1_score(y_test, y_pred, average='macro')\n",
    "        print('log reg : ', log, flog)\n",
    "\n",
    "        mod.fit(np.array(X_train), np.array(y_train))\n",
    "        y_pred = mod.predict(np.array(X_test))\n",
    "        xgb = accuracy_score(y_test, y_pred) \n",
    "        fxgb = f1_score(y_test, y_pred, average='macro')\n",
    "        print('XGBoost : ', xgb, fxgb)\n",
    "\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = np.array(X_train).reshape((np.array(X_train).shape[0], 1,np.array(X_train).shape[1]))\n",
    "        test_X = np.array(X_test).reshape((np.array(X_test).shape[0], 1, np.array(X_test).shape[1]))\n",
    "\n",
    "\n",
    "        Model = Sequential()\n",
    "        Model.add(LSTM(180, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        Model.add(LSTM(units=90, return_sequences=True))\n",
    "        #model.add(BatchNormalization())\n",
    "        Model.add(LSTM(units=30))\n",
    "        Model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "        Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        Model.fit(train_X,np.array(y_train), epochs=10, batch_size=32,shuffle = False,validation_split=0.2, verbose=0)\n",
    "\n",
    "        y_pred = Model.predict(np.array(test_X), verbose=0)\n",
    "\n",
    "        lstm = accuracy_score(y_test, y_pred.round()) \n",
    "        flstm = f1_score(y_test, y_pred.round(), average='macro')\n",
    "        \n",
    "        print('LSTM : ', lstm, flstm)\n",
    "\n",
    "        SVM.append(SVm)\n",
    "        LOG.append(log)\n",
    "        XGB.append(xgb)\n",
    "        LSTm.append(lstm)\n",
    "        \n",
    "        fSVM.append(fSVm)\n",
    "        fLOG.append(flog)\n",
    "        fXGB.append(fxbg)\n",
    "        fLSTm.append(flstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(SVM))\n",
    "code = Codes[1:54] + Codes[55:100]\n",
    "results = pd.DataFrame({'SVM' : SVM, 'fSVM' : fSVM, 'Logistic reg' : LOG, 'flog reg' : fLOG, 'XBGoost' : XGB, 'fXGB' : fXGB,\n",
    "                        'LSTM': LSTm, 'flstm' : fLSTm}, index = code)\n",
    "results.to_csv('RESULTS/S+T.csv')\n",
    "\n",
    "print('Mean')\n",
    "print(results.mean())\n",
    "print('Variance')\n",
    "print(results.var())\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1252 1252 1252\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1470 1470 1470\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "94 94 94\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1312 1312 1312\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n"
     ]
    }
   ],
   "source": [
    "delay = 10\n",
    "days = 10    \n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "binary_train = []\n",
    "binary_test = []\n",
    "ALL = []\n",
    "\n",
    "\n",
    "for code in Codes[1:]:\n",
    "\n",
    "    DF = pd.read_csv('all_data/' + code + '.csv')\n",
    "\n",
    "    days = 10\n",
    "\n",
    "    DF['BERT combined'] = DF['Bert'][::1].rolling(days, min_periods=0).sum()[::-1]\n",
    "\n",
    "    DF = DF.drop(['Bert','neg', 'neu','pos'], axis=1)\n",
    "    \n",
    "    DF = DF.dropna()\n",
    "    \n",
    "    \n",
    "    Brokers = DF[['Buy1','Outperform1', 'Hold1', 'Underperform1', 'Sell1', 'BERT combined']].values\n",
    "\n",
    "    Stock = DF[['Open', 'High', 'Low', 'Close', 'Adj Close','Volume']].values\n",
    "    \n",
    "    windows, binary_pred, pred = window(Stock, Brokers, 10,10)\n",
    "    \n",
    "    print(len(windows),len(pred), len(binary_pred))\n",
    "    \n",
    "    y_train = y_train + pred[:1000]\n",
    "    y_test = y_test + pred[1000:]\n",
    "    \n",
    "    binary_train = binary_train + binary_pred[:1000]\n",
    "    binary_test = binary_test + binary_pred[1000:]\n",
    "        \n",
    "    X_train = X_train + windows[:1000]\n",
    "    X_test = X_test + windows[1000:]\n",
    "    \n",
    "    ALL = ALL + binary_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log reg :  0.5104265139298226 0.33840914425369206\n",
      "XGBoost :  0.5144488313036386 0.43840480196720266\n",
      "LSTM :  0.5111494188956236 0.3408151557425376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, binary_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "log = accuracy_score(binary_test, y_pred) \n",
    "flog = f1_score(binary_test, y_pred, average='macro')\n",
    "print('log reg : ', log, flog)\n",
    "\n",
    "mod = XGBClassifier()\n",
    "mod.fit(np.array(X_train), np.array(binary_train))\n",
    "y_pred = mod.predict(np.array(X_test))\n",
    "xgb = accuracy_score(binary_test, y_pred) \n",
    "fxgb = f1_score(binary_test, y_pred, average='macro')\n",
    "print('XGBoost : ', xgb, fxgb)\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = np.array(X_train).reshape((np.array(X_train).shape[0], 1,np.array(X_train).shape[1]))\n",
    "test_X = np.array(X_test).reshape((np.array(X_test).shape[0], 1, np.array(X_test).shape[1]))\n",
    "\n",
    "Model = Sequential()\n",
    "Model.add(LSTM(180, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "Model.add(LSTM(units=90, return_sequences=True))\n",
    "#Model.add(BatchNormalization())\n",
    "Model.add(LSTM(units=30))\n",
    "Model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "Model.fit(train_X,np.array(binary_train), epochs=1, batch_size=32, shuffle = False, validation_split=0.2, verbose=0)\n",
    "\n",
    "y_pred = Model.predict(np.array(test_X), verbose=0)\n",
    "\n",
    "lstm = accuracy_score(binary_test, y_pred.round()) \n",
    "flstm = f1_score(binary_test, y_pred.round(), average='macro')        \n",
    "print('LSTM : ', lstm, flstm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S T B\n",
    "log reg :  0.5112606350442084 0.34941544900102334\n",
    "XGBoost :  0.5122430443567073 0.4616196911442263\n",
    "LSTM :  0.5103709058555302 0.33791097529546044\n",
    "\n",
    "S T\n",
    "log reg :  0.5122986524309997 0.3832466908666927\n",
    "XGBoost :  0.5074236779180337 0.4856619682102835\n",
    "LSTM :  0.5103709058555302 0.33791097529546044\n",
    "\n",
    "S B\n",
    "log reg :  0.510760162375577 0.3464103395497862\n",
    "XGBoost :  0.5078314704628445 0.44996157555432986\n",
    "LSTM :  0.5103709058555302 0.33791097529546044\n",
    "\n",
    "S\n",
    "log reg :  0.5109269865984541 0.3774555105464708\n",
    "XGBoost :  0.504587666129122 0.48119366636018585\n",
    "LSTM :  0.510352369830766 0.33800445440691235\n",
    "\n",
    "B\n",
    "log reg :  0.510352369830766 0.33797059209975344\n",
    "XGBoost :  0.513633246214017 0.4100036946837216\n",
    "LSTM :  0.5103709058555302 0.33791097529546044\n",
    "\n",
    "T\n",
    "log reg :  0.510630410202228 0.3387015745709705\n",
    "XGBoost :  0.5160429294333537 0.435298519919782\n",
    "LSTM :  0.5126508369015181 0.3507383333606752\n",
    "\n",
    "B T\n",
    "log reg :  0.5104265139298226 0.33840914425369206\n",
    "XGBoost :  0.5144488313036386 0.43840480196720266\n",
    "LSTM :  0.5111494188956236 0.3408151557425376"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
