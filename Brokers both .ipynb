{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the brokers opinion data\n",
    "In this document I process the data collected and model it both by itself and with the data of the stock prices.\n",
    "\n",
    "<br>\n",
    "\n",
    "Overview, the use of the brokers opinion data does indeed result in improved performance than using just the stock price data. I also appears that the use of the brokers data alone is better than using the stock price data with or without the brokers data. And individual models perform better than collective models.\n",
    "This is promising that the use of news articles in the third part of the project will give even better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime as dt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Lambda, Dropout, BatchNormalization\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing \n",
    "This section of the code produces the data that can easily be modelled on. \n",
    "\n",
    "<br>\n",
    "\n",
    "This fist part of the code creates a list of the names of the FTSE 100 companies that are used to open the stock price data for each of the companies. And opens the brokers opinion data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "File = open('stocks_name.txt','r')\n",
    "\n",
    "Name = []\n",
    "Codes = []\n",
    "Type = []\n",
    "Add = []\n",
    "\n",
    "for line in File:\n",
    "    line = re.sub('\\n', '', line)\n",
    "    line = line.split('\\t')\n",
    "    Name.append(re.sub(' ', ' ',line[0].lower())+' ')\n",
    "    Type.append(line[2])\n",
    "    Add.append(line[1].lower()+ ' ')\n",
    "    if line[1][2:4] == '.A':\n",
    "        Codes.append('BT-A.L')\n",
    "    elif line[1][2] == '.':\n",
    "        Codes.append(line[1] + 'L')\n",
    "    else:\n",
    "        Codes.append(line[1] + '.L')\n",
    "\n",
    "\n",
    "file = open('Brokers3.csv','r')\n",
    "\n",
    "number_per = []\n",
    "lines = []\n",
    "for line in file:\n",
    "    line = line.split('], ')\n",
    "    number_per.append(len(line))\n",
    "    lines.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code splits the data from the brokers data into 5 categories; the date the opinion was released, the company that released the opinion, the opinion ('Buy', 'Sell', ect.), the previously predicted price that the stock will go to, the current predicted future price, and the class that is if this is a reiteration, downgrade or upgrade based on previous opinions. \n",
    "\n",
    "<br>\n",
    "In this project I am only concerned with the opinion and the date that the opinion was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" 'Neutral'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Overweight'\", \" 'Buy'\", \" 'Buy'\", \" 'Equal-weight'\", \" 'Neutral'\", \" 'Overweight'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Overweight'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Overweight'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Buy'\", \" 'Overweight'\", \" 'Overweight'\", \" 'Buy'\", \" 'Outperform'\", \" 'Equal-weight'\", \" 'Overweight'\", \" 'Neutral'\", \" 'Overweight'\", \" 'Outperform'\", \" 'Outperform'\", \" 'Buy'\", \" 'Overweight'\", \" 'Buy'\", \" 'Overweight'\", \" 'Buy'\", \" 'Overweight'\", \" 'Buy'\", \" 'Overweight'\", \" 'Buy'\", \" 'Buy'\", \" 'Overweight'\", \" 'Buy'\", \" 'Buy'\", \" 'Overweight'\", \" 'Hold'\", \" 'Buy'\", \" 'Neutral'\", \" 'Neutral'\", \" 'Buy'\", \" 'Buy'\", \" 'Hold'\", \" 'Equal-weight'\", \" 'Buy'\", \" 'Hold'\", \" 'Overweight'\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dates = []\n",
    "companies= []\n",
    "opinions = []\n",
    "pre_preds = []\n",
    "preds = []\n",
    "Classes = []\n",
    "\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    \n",
    "    date = []\n",
    "    company = []\n",
    "    opinion = []\n",
    "    pre_pred = []\n",
    "    pred = []\n",
    "    Class = []\n",
    "    \n",
    "    for j in range(len(lines[i])):\n",
    "        \n",
    "        date.append(re.sub('\\[', '', lines[i][j].split(',')[0]))\n",
    "        company.append(lines[i][j].split(',')[1])\n",
    "        opinion.append(lines[i][j].split(',')[2])\n",
    "        pre_pred.append(lines[i][j].split(',')[3])\n",
    "        pred.append(lines[i][j].split(',')[4])\n",
    "        Class.append(lines[i][j].split(',')[5])\n",
    "        \n",
    "    dates.append(date)\n",
    "    companies.append(company)\n",
    "    opinions.append(opinion)\n",
    "    pre_preds.append(pre_pred)\n",
    "    preds.append(pred)\n",
    "    Classes.append(Class)\n",
    "print(opinions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a list of the opinions, \n",
    "I group data into 5 categories based on the implied meaning of the name,<br>\n",
    "-- sell-(sell, strong sell)<br>\n",
    "-- underperform-(underperform, moderate sell, weak sell, underwieght, reduce)<br>\n",
    "-- hold -(hold, neutral,equal weight)<br>\n",
    "-- outperform-(moderate buy, accumulate, add, over-weight, add,House Stock, Sector Perform,Outperform)<br>\n",
    "-- buy-(buy, strong buy,Top Pick)<br> \n",
    "This is done so an appropriate number of instances of these events can be seen in my training and testing data set.\n",
    "\n",
    "I also keep the data in a 100 element list one for each of the FTSE 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples : \n",
      "['Hold', 'Buy', 'Buy', 'Buy', 'Buy', 'Buy', 'Outperform', 'Buy', 'Buy', 'Hold', 'Hold', 'Outperform', 'Buy', 'Buy', 'Buy', 'Outperform', 'Buy', 'Buy', 'Buy', 'Buy', 'Buy', 'Buy', 'Buy', 'Outperform', 'Buy', 'Buy', 'Buy', 'Buy', 'Buy', 'Outperform', 'Outperform', 'Buy', 'Outperform', 'Hold', 'Outperform', 'Hold', 'Outperform', 'Outperform', 'Outperform', 'Buy', 'Outperform', 'Buy', 'Outperform', 'Buy', 'Outperform', 'Buy', 'Outperform', 'Buy', 'Buy', 'Outperform', 'Buy', 'Buy', 'Outperform', 'Hold', 'Buy', 'Hold', 'Hold', 'Buy', 'Buy', 'Hold', 'Hold', 'Buy', 'Hold', 'Outperform']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Views = []\n",
    "for i in range(100):\n",
    "    \n",
    "    Opinions = []\n",
    "    \n",
    "    for op in opinions[i]:\n",
    "        op = op[2:-1]\n",
    "        \n",
    "        if op == 'Buy':\n",
    "            Opinions.append('Buy')\n",
    "        elif op == 'Top Pick':\n",
    "            Opinions.append('Buy')\n",
    "        elif op == 'Strong Buy':\n",
    "            Opinions.append('Buy')\n",
    "            \n",
    "        elif op == 'Overweight':\n",
    "            Opinions.append('Outperform')\n",
    "        elif op == 'Add':\n",
    "            Opinions.append('Outperform')\n",
    "        elif op == 'House Stock':\n",
    "            Opinions.append('Outperform')\n",
    "        elif op == 'Sector Perform':\n",
    "            Opinions.append('Outperform')\n",
    "        elif op == 'Outperform':\n",
    "            Opinions.append('Outperform')\n",
    "        elif op == 'Accumulate':\n",
    "            Opinions.append('Outperform')\n",
    "        elif op == 'Speculative Buy':\n",
    "            Opinions.append('Outperform')\n",
    "        elif op == 'Market Perform':\n",
    "            Opinions.append('Outperform')\n",
    "        elif op == 'Corporate':\n",
    "            Opinions.append('Outperform')\n",
    "                        \n",
    "        elif op == 'Hold':\n",
    "            Opinions.append('Hold')\n",
    "        elif op == 'Neutral':\n",
    "            Opinions.append('Hold')\n",
    "        elif op == 'Equal-weight':\n",
    "            Opinions.append('Hold')\n",
    "        elif op == 'Not Rated':\n",
    "            Opinions.append('Hold')\n",
    "        elif op == 'arnier & Co':\n",
    "            Opinions.append('Hold')\n",
    "        elif op == 'In-Line':\n",
    "            Opinions.append('Hold')\n",
    "            \n",
    "        elif op == 'Underperform':\n",
    "            Opinions.append('Underperform')\n",
    "        elif op == 'Underweight':\n",
    "            Opinions.append('Underperform')\n",
    "        elif op == 'Reduce':\n",
    "            Opinions.append('Underperform')\n",
    "            \n",
    "        elif op == 'Sell':\n",
    "            Opinions.append('Sell')\n",
    "        elif op == 'Suspended':\n",
    "            Opinions.append('Sell')\n",
    "        elif op == 'Strong Sell':\n",
    "            Opinions.append('Sell')\n",
    "            \n",
    "        else:\n",
    "            print('Error ', op, 'not recognised!')\n",
    "        \n",
    "    Views.append(Opinions)\n",
    "print('Examples : ')\n",
    "print(Views[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I wish to make a list that contains the date and the opinion in a one-hot-encoded way. As can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Buy</th>\n",
       "      <th>Outperform</th>\n",
       "      <th>Hold</th>\n",
       "      <th>Underperform</th>\n",
       "      <th>Sell</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-03</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-05</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-24</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Buy  Outperform  Hold  Underperform  Sell\n",
       "Date                                                 \n",
       "2014-01-13    0           0     1             0     0\n",
       "2014-01-15    0           0     0             1     0\n",
       "2014-01-31    0           0     0             1     0\n",
       "2014-02-03    1           0     0             0     0\n",
       "2014-02-05    0           0     1             0     0\n",
       "2014-02-11    1           0     0             0     0\n",
       "2014-02-19    1           0     0             0     0\n",
       "2014-02-20    1           0     0             0     0\n",
       "2014-02-21    1           0     0             0     0\n",
       "2014-02-24    0           1     1             0     0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BROKER = []\n",
    "\n",
    "for target in range(100):\n",
    "    \n",
    "    dat = dates[target]\n",
    "    da = []\n",
    "    for d in dat:\n",
    "        d = d[1:-1]\n",
    "        da.append(datetime.datetime.strptime(d, '%d-%b-%y'))\n",
    "\n",
    "\n",
    "    Buy = []\n",
    "    Outperform = []\n",
    "    Hold = []\n",
    "    Underperform = []\n",
    "    Sell = []\n",
    "    for i in range(len(dat)):\n",
    "        if Views[target][i] == 'Buy':\n",
    "            Buy.append(1)\n",
    "            Outperform.append(0)\n",
    "            Hold.append(0)\n",
    "            Underperform.append(0)\n",
    "            Sell.append(0)\n",
    "        elif Views[target][i] == 'Outperform':\n",
    "            Buy.append(0)\n",
    "            Outperform.append(1)\n",
    "            Hold.append(0)\n",
    "            Underperform.append(0)\n",
    "            Sell.append(0)\n",
    "        elif Views[target][i] == 'Hold':\n",
    "            Buy.append(0)\n",
    "            Outperform.append(0)\n",
    "            Hold.append(1)\n",
    "            Underperform.append(0)\n",
    "            Sell.append(0)\n",
    "        elif Views[target][i] == 'Underperform':\n",
    "            Buy.append(0)\n",
    "            Outperform.append(0)\n",
    "            Hold.append(0)\n",
    "            Underperform.append(1)\n",
    "            Sell.append(0)\n",
    "        elif Views[target][i] == 'Sell':\n",
    "            Buy.append(0)\n",
    "            Outperform.append(0)\n",
    "            Hold.append(0)\n",
    "            Underperform.append(0)\n",
    "            Sell.append(1)\n",
    "            \n",
    "    data = []\n",
    "    for i in range(len(dat)):\n",
    "        data.append([da[i], Buy[i], Outperform[i], Hold[i], Underperform[i], Sell[i]])\n",
    "        \n",
    "    BROKER.append(data)\n",
    "\n",
    "\n",
    "DF = pd.DataFrame(BROKER[10], columns=['Date','Buy', 'Outperform', 'Hold', 'Underperform', 'Sell'])\n",
    "DF = DF.sort_values('Date')\n",
    "DF.set_index('Date', inplace=True)\n",
    "DF = DF.truncate(before='2014-01-02')\n",
    "DF = DF.groupby(DF.index).sum()\n",
    "DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now I wish to create a data frame that contains the opinions and matches them up with the date they where released. This creates a data point one per day so if two articles are released on the same day they become one point in my data frame, as seen above.\n",
    "<br>\n",
    "\n",
    "As I do not have any stock price data for before 2014 I remove all the brokers data from before then this is about 5% of the data.\n",
    "<br>\n",
    "\n",
    "I then go on to open the stock price data and join the two data frames together based on the date. And fill in all the missing data form the brokers data set as they aren't released as often as the stock data is.\n",
    "\n",
    "<br>\n",
    "\n",
    "As the brokers opinions are relevant for more than just the day they are released. This is because they are longer term predictions for the price, I propagate the data through the data frame. The amount of time this is done for is given by the variable day, and is currently set at 100 days. \n",
    "\n",
    "<br>\n",
    "An example of the data processing can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Buy</th>\n",
       "      <th>Outperform</th>\n",
       "      <th>Hold</th>\n",
       "      <th>Underperform</th>\n",
       "      <th>Sell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>496.100006</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>14690819.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>505.600006</td>\n",
       "      <td>537.599976</td>\n",
       "      <td>498.399994</td>\n",
       "      <td>537.599976</td>\n",
       "      <td>537.599976</td>\n",
       "      <td>16495524.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>523.599976</td>\n",
       "      <td>523.599976</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>516.200012</td>\n",
       "      <td>516.200012</td>\n",
       "      <td>8943020.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>514.599976</td>\n",
       "      <td>520.200012</td>\n",
       "      <td>498.100006</td>\n",
       "      <td>508.399994</td>\n",
       "      <td>508.399994</td>\n",
       "      <td>7855795.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>513.799988</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>498.598999</td>\n",
       "      <td>521.799988</td>\n",
       "      <td>521.799988</td>\n",
       "      <td>12679800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Open        High         Low       Close   Adj Close      Volume  \\\n",
       "1573  496.100006  523.599976  480.000000  523.599976  523.599976  14690819.0   \n",
       "1574  505.600006  537.599976  498.399994  537.599976  537.599976  16495524.0   \n",
       "1575  523.599976  523.599976  499.000000  516.200012  516.200012   8943020.0   \n",
       "1576  514.599976  520.200012  498.100006  508.399994  508.399994   7855795.0   \n",
       "1577  513.799988  523.000000  498.598999  521.799988  521.799988  12679800.0   \n",
       "\n",
       "      Buy  Outperform  Hold  Underperform  Sell  \n",
       "1573  2.0         1.0   1.0           0.0   0.0  \n",
       "1574  2.0         1.0   1.0           0.0   0.0  \n",
       "1575  2.0         1.0   1.0           0.0   0.0  \n",
       "1576  2.0         1.0   1.0           0.0   0.0  \n",
       "1577  2.0         1.0   1.0           0.0   0.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = []\n",
    "\n",
    "for target in range(len(Codes)):\n",
    "    \n",
    "    DF = pd.DataFrame(BROKER[target], columns=['Date','Buy', 'Outperform', 'Hold', 'Underperform', 'Sell'])\n",
    "    DF = DF.sort_values('Date')\n",
    "    DF.set_index('Date', inplace=True)\n",
    "    DF = DF.truncate(before='2014-01-02')\n",
    "    # Deals with two opinions being posted on the same day\n",
    "    DF = DF.groupby(DF.index).sum()\n",
    "\n",
    "    df = pd.read_csv('csv/' + Codes[target] + '.csv')\n",
    "    # Sort DataFrame by date\n",
    "    df = df.sort_values('Date')\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Joining the datasets \n",
    "    broker_data = df.join(DF, how='outer')\n",
    "    broker_data.fillna(0)\n",
    "    \n",
    "    ###################\n",
    "    days = 100\n",
    "    ###################\n",
    "    \n",
    "    broker_data['Buy1'] = broker_data['Buy'][::1].rolling(days, min_periods=0).sum()[::-1]\n",
    "    broker_data['Outperform1'] = broker_data['Outperform'][::1].rolling(days, min_periods=0).sum()[::-1]\n",
    "    broker_data['Hold1'] =broker_data['Hold'][::1].rolling(days, min_periods=0).sum()[::-1]\n",
    "    broker_data['Underperform1'] = broker_data['Underperform'][::1].rolling(days, min_periods=0).sum()[::-1]\n",
    "    broker_data['Sell1'] = broker_data['Sell'][::1].rolling(days, min_periods=0).sum()[::-1]\n",
    "\n",
    "    broker_data = broker_data.drop(columns=['Buy','Outperform','Hold','Underperform', 'Sell'])\n",
    "    \n",
    "    DATA.append(broker_data.values)\n",
    "\n",
    "\n",
    "DF = pd.DataFrame(DATA[10], columns=['Open', 'High', 'Low', 'Close', 'Adj Close','Volume','Buy', 'Outperform', 'Hold', 'Underperform', 'Sell'])\n",
    "DF.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window function\n",
    "This function creates overlapping windows of the data. That is it takes 'size' number of days (the size that the window should be) for each data point. It then takes the future value of the stock and compares the change from the last day of the data window. I will be using the closing price as the value I am trying to predict.\n",
    "\n",
    "<br>\n",
    "I have allowed for both using and not using scaling of the data by either using or removing the scaling from this function.\n",
    "I also allow for the use of just the brokers data or just the stock price data or both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "def window(stock, brokers, size, delay):\n",
    "    \n",
    "    windows = []\n",
    "    binary_pred = []\n",
    "    pred = []\n",
    "        \n",
    "    \n",
    "    for i in range(len(stock) - delay - size):\n",
    "        \n",
    "        scaler.fit(stock[i: i + size])\n",
    "        \n",
    "        win = []\n",
    "        for j in range(size):\n",
    "            # comment top line to only include brokers opinions\n",
    "            #win.append(np.concatenate(scaler.transform(stock[i+j:i+j+1])))\n",
    "            win.append(brokers[i+j])\n",
    "            \n",
    "        windows.append(np.concatenate(win))\n",
    "        \n",
    "        pred.append(scaler.transform(stock[i+size:i + size + delay])[-1][-3] - scaler.transform(stock[i:i + size])[-1][-3]) # The actual value scaled appropriatly\n",
    "        \n",
    "        #Binary pred catagory\n",
    "        if scaler.transform(stock[i+size:i + size + delay])[-1][-3] < scaler.transform(stock[i:i + size])[-1][-3]:\n",
    "            binary_pred.append(1)\n",
    "        else:\n",
    "            binary_pred.append(0)\n",
    "    \n",
    "        \n",
    "    return windows, binary_pred, pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets look at using the stock price and brokers for each company\n",
    "This part of the code models the data. I use four models for this part; SVM, logistic regression, XGBoost and an LSTM.\n",
    "\n",
    "<br>\n",
    "To avoid my model form being able to see future data and to keep the time series features I do not shuffle the data and I also do not use conventional validation techniques such as k-fold validation. I also train on the first 1000 data point that represents about 4.5 years of my data and test on 1.5 years \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split :  0.9774011299435028\n",
      "SVM :  0.9774011299435028\n",
      "log reg :  0.9698681732580038\n",
      "XGBoost :  0.9378531073446328\n",
      "LSTM :  0.9774011373519897\n",
      "Split :  0.5350089766606823\n",
      "SVM :  0.5314183123877917\n",
      "log reg :  0.4865350089766607\n",
      "XGBoost :  0.4631956912028725\n",
      "LSTM :  0.53500896692276\n",
      "Split :  0.5116696588868941\n",
      "SVM :  0.5116696588868941\n",
      "log reg :  0.5152603231597845\n",
      "XGBoost :  0.5314183123877917\n",
      "LSTM :  0.5116696357727051\n",
      "Split :  0.4793536804308797\n",
      "SVM :  0.4793536804308797\n",
      "log reg :  0.48473967684021546\n",
      "XGBoost :  0.4793536804308797\n",
      "LSTM :  0.479353666305542\n",
      "Split :  0.526032315978456\n",
      "SVM :  0.5278276481149012\n",
      "log reg :  0.5224416517055656\n",
      "XGBoost :  0.4542190305206463\n",
      "LSTM :  0.5170556306838989\n",
      "Split :  0.5008976660682226\n",
      "SVM :  0.4129263913824057\n",
      "log reg :  0.5116696588868941\n",
      "XGBoost :  0.4793536804308797\n",
      "LSTM :  0.5008976459503174\n",
      "Split :  0.5080789946140036\n",
      "SVM :  0.4919210053859964\n",
      "log reg :  0.5044883303411131\n",
      "XGBoost :  0.5224416517055656\n",
      "LSTM :  0.5727109313011169\n",
      "Split :  0.4642857142857143\n",
      "SVM :  0.4642857142857143\n",
      "log reg :  0.5436507936507936\n",
      "XGBoost :  0.47619047619047616\n",
      "LSTM :  0.5833333134651184\n",
      "Split :  0.6265709156193896\n",
      "SVM :  0.6265709156193896\n",
      "log reg :  0.6463195691202872\n",
      "XGBoost :  0.5709156193895871\n",
      "LSTM :  0.6265709400177002\n",
      "Split :  0.4488330341113106\n",
      "SVM :  0.4488330341113106\n",
      "log reg :  0.4631956912028725\n",
      "XGBoost :  0.45601436265709155\n",
      "LSTM :  0.46499103307724\n",
      "Split :  0.5457809694793537\n",
      "SVM :  0.5134649910233393\n",
      "log reg :  0.5350089766606823\n",
      "XGBoost :  0.5511669658886894\n",
      "LSTM :  0.5457809567451477\n",
      "Split :  0.4506283662477558\n",
      "SVM :  0.5493716337522442\n",
      "log reg :  0.547576301615799\n",
      "XGBoost :  0.4883303411131059\n",
      "LSTM :  0.4937163293361664\n",
      "Split :  0.5278276481149012\n",
      "SVM :  0.5008976660682226\n",
      "log reg :  0.5224416517055656\n",
      "XGBoost :  0.533213644524237\n",
      "LSTM :  0.5475763082504272\n",
      "Split :  0.5385996409335727\n",
      "SVM :  0.526032315978456\n",
      "log reg :  0.4272890484739677\n",
      "XGBoost :  0.45960502692998206\n",
      "LSTM :  0.5385996699333191\n",
      "Split :  0.473967684021544\n",
      "SVM :  0.526032315978456\n",
      "log reg :  0.526032315978456\n",
      "XGBoost :  0.526032315978456\n",
      "LSTM :  0.47396767139434814\n",
      "Split :  0.4326750448833034\n",
      "SVM :  0.4326750448833034\n",
      "log reg :  0.4362657091561939\n",
      "XGBoost :  0.5206463195691203\n",
      "LSTM :  0.4326750338077545\n",
      "Split :  0.4506283662477558\n",
      "SVM :  0.4506283662477558\n",
      "log reg :  0.45601436265709155\n",
      "XGBoost :  0.4757630161579892\n",
      "LSTM :  0.4506283700466156\n",
      "Split :  0.4542190305206463\n",
      "SVM :  0.4703770197486535\n",
      "log reg :  0.45601436265709155\n",
      "XGBoost :  0.46858168761220825\n",
      "LSTM :  0.4542190432548523\n",
      "Split :  0.43087971274685816\n",
      "SVM :  0.4883303411131059\n",
      "log reg :  0.5206463195691203\n",
      "XGBoost :  0.5134649910233393\n",
      "LSTM :  0.43087971210479736\n",
      "Split :  0.5242369838420108\n",
      "SVM :  0.5116696588868941\n",
      "log reg :  0.5134649910233393\n",
      "XGBoost :  0.5547576301615799\n",
      "LSTM :  0.5242369771003723\n",
      "Split :  0.518850987432675\n",
      "SVM :  0.547576301615799\n",
      "log reg :  0.47217235188509876\n",
      "XGBoost :  0.48473967684021546\n",
      "LSTM :  0.5188509821891785\n",
      "Split :  0.47217235188509876\n",
      "SVM :  0.47217235188509876\n",
      "log reg :  0.48114901256732495\n",
      "XGBoost :  0.5385996409335727\n",
      "LSTM :  0.472172349691391\n",
      "Split :  0.42010771992818674\n",
      "SVM :  0.47755834829443444\n",
      "log reg :  0.49012567324955114\n",
      "XGBoost :  0.4021543985637343\n",
      "LSTM :  0.44524237513542175\n",
      "Split :  0.5511669658886894\n",
      "SVM :  0.4865350089766607\n",
      "log reg :  0.5529622980251346\n",
      "XGBoost :  0.4793536804308797\n",
      "LSTM :  0.5511669516563416\n",
      "Split :  0.547576301615799\n",
      "SVM :  0.5170556552962298\n",
      "log reg :  0.533213644524237\n",
      "XGBoost :  0.5583482944344704\n",
      "LSTM :  0.5475763082504272\n",
      "Split :  0.526032315978456\n",
      "SVM :  0.526032315978456\n",
      "log reg :  0.49012567324955114\n",
      "XGBoost :  0.48473967684021546\n",
      "LSTM :  0.5260323286056519\n",
      "Split :  0.5529622980251346\n",
      "SVM :  0.5673249551166966\n",
      "log reg :  0.5529622980251346\n",
      "XGBoost :  0.4578096947935368\n",
      "LSTM :  0.5529623031616211\n",
      "Split :  0.4578096947935368\n",
      "SVM :  0.4578096947935368\n",
      "log reg :  0.45601436265709155\n",
      "XGBoost :  0.47217235188509876\n",
      "LSTM :  0.4578096866607666\n",
      "Split :  0.5368043087971275\n",
      "SVM :  0.44703770197486536\n",
      "log reg :  0.5403949730700179\n",
      "XGBoost :  0.5080789946140036\n",
      "LSTM :  0.5368043184280396\n",
      "Split :  0.49012567324955114\n",
      "SVM :  0.49012567324955114\n",
      "log reg :  0.49730700179533216\n",
      "XGBoost :  0.47755834829443444\n",
      "LSTM :  0.5511669516563416\n",
      "Split :  0.5008976660682226\n",
      "SVM :  0.5008976660682226\n",
      "log reg :  0.5098743267504489\n",
      "XGBoost :  0.5026929982046678\n",
      "LSTM :  0.5008976459503174\n",
      "Split :  0.6409335727109515\n",
      "SVM :  0.5385996409335727\n",
      "log reg :  0.39676840215439857\n",
      "XGBoost :  0.5116696588868941\n",
      "LSTM :  0.6409335732460022\n",
      "Split :  0.5745062836624776\n",
      "SVM :  0.5745062836624776\n",
      "log reg :  0.5745062836624776\n",
      "XGBoost :  0.5745062836624776\n",
      "LSTM :  0.5745062828063965\n",
      "Split :  0.4829443447037702\n",
      "SVM :  0.4829443447037702\n",
      "log reg :  0.4829443447037702\n",
      "XGBoost :  0.4829443447037702\n",
      "LSTM :  0.4829443395137787\n",
      "Split :  0.578096947935368\n",
      "SVM :  0.42190305206463197\n",
      "log reg :  0.5673249551166966\n",
      "XGBoost :  0.5278276481149012\n",
      "LSTM :  0.5798922777175903\n",
      "Split :  0.4614003590664273\n",
      "SVM :  0.5583482944344704\n",
      "log reg :  0.5457809694793537\n",
      "XGBoost :  0.4865350089766607\n",
      "LSTM :  0.45242369174957275\n",
      "Split :  0.6140035906642729\n",
      "SVM :  0.5870736086175943\n",
      "log reg :  0.5350089766606823\n",
      "XGBoost :  0.5583482944344704\n",
      "LSTM :  0.614003598690033\n",
      "Split :  0.5601436265709157\n",
      "SVM :  0.533213644524237\n",
      "log reg :  0.5601436265709157\n",
      "XGBoost :  0.547576301615799\n",
      "LSTM :  0.5475763082504272\n",
      "Split :  0.4254937163375224\n",
      "SVM :  0.45960502692998206\n",
      "log reg :  0.4631956912028725\n",
      "XGBoost :  0.4955116696588869\n",
      "LSTM :  0.5008976459503174\n",
      "Split :  0.5565529622980251\n",
      "SVM :  0.44344703770197486\n",
      "log reg :  0.44165170556552963\n",
      "XGBoost :  0.5296229802513465\n",
      "LSTM :  0.5565529465675354\n",
      "Split :  0.4111310592459605\n",
      "SVM :  0.40574506283662476\n",
      "log reg :  0.4398563734290844\n",
      "XGBoost :  0.42190305206463197\n",
      "LSTM :  0.5565529465675354\n",
      "Split :  0.5224416517055656\n",
      "SVM :  0.5368043087971275\n",
      "log reg :  0.4236983842010772\n",
      "XGBoost :  0.4129263913824057\n",
      "LSTM :  0.5278276205062866\n",
      "Split :  0.518850987432675\n",
      "SVM :  0.5116696588868941\n",
      "log reg :  0.4649910233393178\n",
      "XGBoost :  0.5763016157989228\n",
      "LSTM :  0.5188509821891785\n",
      "Split :  0.49730700179533216\n",
      "SVM :  0.5134649910233393\n",
      "log reg :  0.5798922800718133\n",
      "XGBoost :  0.4865350089766607\n",
      "LSTM :  0.5152603387832642\n",
      "Split :  0.5116696588868941\n",
      "SVM :  0.5116696588868941\n",
      "log reg :  0.5080789946140036\n",
      "XGBoost :  0.5116696588868941\n",
      "LSTM :  0.5116696357727051\n",
      "Split :  0.4524236983842011\n",
      "SVM :  0.48114901256732495\n",
      "log reg :  0.4919210053859964\n",
      "XGBoost :  0.518850987432675\n",
      "LSTM :  0.45242369174957275\n",
      "Split :  0.6032315978456014\n",
      "SVM :  0.6032315978456014\n",
      "log reg :  0.5547576301615799\n",
      "XGBoost :  0.5583482944344704\n",
      "LSTM :  0.6032316088676453\n",
      "Split :  0.49371633752244165\n",
      "SVM :  0.47217235188509876\n",
      "log reg :  0.5008976660682226\n",
      "XGBoost :  0.5044883303411131\n",
      "LSTM :  0.4865350127220154\n",
      "Split :  0.5234042553191489\n",
      "SVM :  0.5276595744680851\n",
      "log reg :  0.4829787234042553\n",
      "XGBoost :  0.3829787234042553\n",
      "LSTM :  0.5234042406082153\n",
      "Split :  0.45960502692998206\n",
      "SVM :  0.5134649910233393\n",
      "log reg :  0.4578096947935368\n",
      "XGBoost :  0.5044883303411131\n",
      "LSTM :  0.45960503816604614\n",
      "Split :  0.48473967684021546\n",
      "SVM :  0.5152603231597845\n",
      "log reg :  0.5421903052064632\n",
      "XGBoost :  0.44344703770197486\n",
      "LSTM :  0.4847396910190582\n",
      "Split :  0.5008976660682226\n",
      "SVM :  0.49730700179533216\n",
      "log reg :  0.48473967684021546\n",
      "XGBoost :  0.45960502692998206\n",
      "LSTM :  0.5008976459503174\n",
      "Split :  0.42190305206463197\n",
      "SVM :  0.5008976660682226\n",
      "log reg :  0.6104129263913824\n",
      "XGBoost :  0.5170556552962298\n",
      "LSTM :  0.5008976459503174\n",
      "Split :  0.6463195691202872\n",
      "SVM :  0.6265709156193896\n",
      "log reg :  0.6265709156193896\n",
      "XGBoost :  0.6211849192100538\n",
      "LSTM :  0.646319568157196\n"
     ]
    }
   ],
   "source": [
    "Svm = svm.SVC(C=1)\n",
    "mod = XGBClassifier()\n",
    "\n",
    "SVM = []\n",
    "LOG = []\n",
    "XGB = []\n",
    "LSTm = []\n",
    "SPLIT = []\n",
    "\n",
    "\n",
    "for i in range(54):\n",
    "\n",
    "    DF = pd.DataFrame(DATA[i], columns=['Open', 'High', 'Low', 'Close', 'Adj Close','Volume','Buy', 'Outperform', 'Hold', 'Underperform', 'Sell'])\n",
    "\n",
    "    DF = DF.dropna()\n",
    "\n",
    "    \n",
    "    Brokers = DF[['Buy','Outperform', 'Hold', 'Underperform', 'Sell']].values\n",
    "\n",
    "    Stock = DF[['Open', 'High', 'Low', 'Close', 'Adj Close','Volume']].values\n",
    "    \n",
    "    \n",
    "    windows, binary_pred, pred = window(Stock, Brokers, 10, 10)\n",
    "    \n",
    "    \n",
    "    X_train = windows[:1000]\n",
    "    X_test = windows[1000:]\n",
    "    y_train = binary_pred[:1000]\n",
    "    y_test = binary_pred[1000:]\n",
    "    \n",
    "    if len(y_test) > 0:\n",
    "    \n",
    "        split = 0\n",
    "        for i in y_test:\n",
    "            if i == 0:\n",
    "                split+=1\n",
    "        SPLIT.append(split/len(y_test))\n",
    "\n",
    "        print('Split : ', split/len(y_test))\n",
    "\n",
    "        Svm.fit(X_train, y_train)\n",
    "        SVm = Svm.score(X_test, y_test) \n",
    "        print('SVM : ', SVm)\n",
    "\n",
    "        clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
    "        log = clf.score(X_test, y_test)\n",
    "        print('log reg : ', log)\n",
    "\n",
    "        mod.fit(np.array(X_train), np.array(y_train))\n",
    "        xgb = mod.score(np.array(X_test), np.array(y_test))\n",
    "        print('XGBoost : ', xgb)\n",
    "\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = np.array(X_train).reshape((np.array(X_train).shape[0], 1,np.array(X_train).shape[1]))\n",
    "        test_X = np.array(X_test).reshape((np.array(X_test).shape[0], 1, np.array(X_test).shape[1]))\n",
    "\n",
    "\n",
    "        Model = Sequential()\n",
    "        Model.add(LSTM(180, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        Model.add(LSTM(units=90, return_sequences=True))\n",
    "        #model.add(BatchNormalization())\n",
    "        Model.add(LSTM(units=30))\n",
    "        Model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "        Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        Model.fit(train_X,np.array(y_train), epochs=10, batch_size=32,shuffle = False,validation_split=0.2, verbose=0)\n",
    "\n",
    "        lstm = Model.evaluate(np.array(test_X),np.array(y_test), verbose=0)[1]\n",
    "\n",
    "        print('LSTM : ', lstm)\n",
    "\n",
    "        SVM.append(SVm)\n",
    "        LOG.append(log)\n",
    "        XGB.append(xgb)\n",
    "        LSTm.append(lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "These are the results for using both the stock price and brokers opinions\n",
    "\n",
    "As can be seen the LSTM does out perform the other models as hoped due to its increased complexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM</th>\n",
       "      <th>Logistic reg</th>\n",
       "      <th>XBGoost</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>III.L</th>\n",
       "      <td>0.977401</td>\n",
       "      <td>0.969868</td>\n",
       "      <td>0.996234</td>\n",
       "      <td>0.979284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADM.L</th>\n",
       "      <td>0.531418</td>\n",
       "      <td>0.488330</td>\n",
       "      <td>0.511670</td>\n",
       "      <td>0.542190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL.L</th>\n",
       "      <td>0.511670</td>\n",
       "      <td>0.511670</td>\n",
       "      <td>0.526032</td>\n",
       "      <td>0.511670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANTO.L</th>\n",
       "      <td>0.479354</td>\n",
       "      <td>0.479354</td>\n",
       "      <td>0.490126</td>\n",
       "      <td>0.479354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AHT.L</th>\n",
       "      <td>0.509874</td>\n",
       "      <td>0.513465</td>\n",
       "      <td>0.513465</td>\n",
       "      <td>0.526032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             SVM  Logistic reg   XBGoost      LSTM\n",
       "III.L   0.977401      0.969868  0.996234  0.979284\n",
       "ADM.L   0.531418      0.488330  0.511670  0.542190\n",
       "AAL.L   0.511670      0.511670  0.526032  0.511670\n",
       "ANTO.L  0.479354      0.479354  0.490126  0.479354\n",
       "AHT.L   0.509874      0.513465  0.513465  0.526032"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(SVM))\n",
    "results = pd.DataFrame({'SVM' : SVM, 'Logistic reg' : LOG, 'XBGoost' : XGB, 'LSTM': LSTm}, index = Codes[:54])\n",
    "#results.to_csv('Brokers_stock_individual.csv')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\n",
      "SVM             0.512024\n",
      "Logistic reg    0.513030\n",
      "XBGoost         0.514192\n",
      "LSTM            0.518968\n",
      "dtype: float64\n",
      "Variance\n",
      "SVM             0.005983\n",
      "Logistic reg    0.005285\n",
      "XBGoost         0.005962\n",
      "LSTM            0.006867\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Mean')\n",
    "print(results.mean())\n",
    "print('Variance')\n",
    "print(results.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the results when using only the brokers opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVM</th>\n",
       "      <th>Logistic reg</th>\n",
       "      <th>XBGoost</th>\n",
       "      <th>LSTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>III.L</th>\n",
       "      <td>0.977401</td>\n",
       "      <td>0.969868</td>\n",
       "      <td>0.937853</td>\n",
       "      <td>0.977401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADM.L</th>\n",
       "      <td>0.531418</td>\n",
       "      <td>0.486535</td>\n",
       "      <td>0.463196</td>\n",
       "      <td>0.535009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL.L</th>\n",
       "      <td>0.511670</td>\n",
       "      <td>0.515260</td>\n",
       "      <td>0.531418</td>\n",
       "      <td>0.511670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANTO.L</th>\n",
       "      <td>0.479354</td>\n",
       "      <td>0.484740</td>\n",
       "      <td>0.479354</td>\n",
       "      <td>0.479354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AHT.L</th>\n",
       "      <td>0.527828</td>\n",
       "      <td>0.522442</td>\n",
       "      <td>0.454219</td>\n",
       "      <td>0.517056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             SVM  Logistic reg   XBGoost      LSTM\n",
       "III.L   0.977401      0.969868  0.937853  0.977401\n",
       "ADM.L   0.531418      0.486535  0.463196  0.535009\n",
       "AAL.L   0.511670      0.515260  0.531418  0.511670\n",
       "ANTO.L  0.479354      0.484740  0.479354  0.479354\n",
       "AHT.L   0.527828      0.522442  0.454219  0.517056"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'SVM' : SVM, 'Logistic reg' : LOG, 'XBGoost' : XGB, 'LSTM': LSTm}, index = Codes[:54])\n",
    "results.to_csv('Brokers_individual.csv')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\n",
      "SVM             0.513861\n",
      "Logistic reg    0.516692\n",
      "XBGoost         0.508576\n",
      "LSTM            0.528887\n",
      "dtype: float64\n",
      "Variance\n",
      "SVM             0.006480\n",
      "Logistic reg    0.006549\n",
      "XBGoost         0.005689\n",
      "LSTM            0.006568\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Mean')\n",
    "print(results.mean())\n",
    "print('Variance')\n",
    "print(results.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now combining all the companies\n",
    "\n",
    "Instead of building one model for each of the companies I combine the data sets. To do this I only take the first 1000 data point of each of the sets leaving the rest for future prediction. This avoids my models being able to see future data of other companies that would not be correct to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531 1531 1531\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1252 1252 1252\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1470 1470 1470\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "94 94 94\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1312 1312 1312\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n",
      "1557 1557 1557\n"
     ]
    }
   ],
   "source": [
    "delay = 10\n",
    "days = 10    \n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "binary_train = []\n",
    "binary_test = []\n",
    "ALL = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    DF = pd.DataFrame(DATA[i], columns=['Open', 'High', 'Low', 'Close', 'Adj Close','Volume','Buy', 'Outperform', 'Hold', 'Underperform', 'Sell'])\n",
    "\n",
    "    DF = DF.dropna()\n",
    "    \n",
    "    Brokers = DF[['Buy','Outperform', 'Hold', 'Underperform', 'Sell']].values\n",
    "\n",
    "    Stock = DF[['Open', 'High', 'Low', 'Close', 'Adj Close','Volume']].values\n",
    "    \n",
    "    windows, binary_pred, pred = window(Stock, Brokers, 10, 10)\n",
    "    \n",
    "    print(len(windows),len(pred), len(binary_pred))\n",
    "    \n",
    "    y_train = y_train + pred[:1000]\n",
    "    y_test = y_test + pred[1000:]\n",
    "    \n",
    "    binary_train = binary_train + binary_pred[:1000]\n",
    "    binary_test = binary_test + binary_pred[1000:]\n",
    "        \n",
    "    X_train = X_train + windows[:1000]\n",
    "    X_test = X_test + windows[1000:]\n",
    "    \n",
    "    ALL = ALL + binary_pred[1000-delay:-delay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99094 99094 99094\n",
      "54480 54480 54480\n",
      "Test :  0.5149229074889868\n",
      "trian :  0.5410922962036047\n",
      "All :  0.50417469492614\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(y_train), len(binary_train))\n",
    "print(len(X_test),len(y_test), len(binary_test))\n",
    "\n",
    "split = 0\n",
    "for i in binary_test:\n",
    "    if i == 0:\n",
    "        split+=1\n",
    "print('Test : ', split/len(binary_test))\n",
    "\n",
    "split = 0\n",
    "for i in binary_train:\n",
    "    if i == 0:\n",
    "        split+=1\n",
    "print('trian : ', split/len(binary_train))\n",
    "\n",
    "split = 0\n",
    "for i in binary_pred:\n",
    "    if i == 0:\n",
    "        split+=1\n",
    "\n",
    "print('All : ', split/len(binary_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log reg :  0.5149045521292217\n",
      "XGBoost :  0.516409691629956\n",
      "Confusion Matrix\n",
      "[[28050     3]\n",
      " [26425     2]]\n",
      "Classification Report\n",
      "Accuracy :  0.5149045521292217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Increase       0.51      1.00      0.68     28053\n",
      "    decrease       0.40      0.00      0.00     26427\n",
      "\n",
      "    accuracy                           0.51     54480\n",
      "   macro avg       0.46      0.50      0.34     54480\n",
      "weighted avg       0.46      0.51      0.35     54480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Svm.fit(X_train, binary_train)\n",
    "#SVm = Svm.score(X_test, binary_test) \n",
    "#print('SVM : ', SVm)\n",
    "    \n",
    "clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, binary_train)\n",
    "print('log reg : ', clf.score(X_test, binary_test))\n",
    "\n",
    "mod = XGBClassifier()\n",
    "mod.fit(np.array(X_train), np.array(binary_train))\n",
    "print('XGBoost : ', mod.score(np.array(X_test), np.array(binary_test)))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "y = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == 0:\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1)\n",
    "        \n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(binary_test, y))\n",
    "print('Classification Report')\n",
    "\n",
    "a=0\n",
    "for i in range(len(y)):\n",
    "    if y[i] == binary_test[i]:\n",
    "        a+=1\n",
    "print('Accuracy : ', a/len(y_pred))\n",
    "               \n",
    "target_names = ['Increase', 'decrease']\n",
    "print(classification_report(binary_test, y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99094, 50)\n",
      "Model: \"sequential_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_337 (LSTM)              (None, 1, 180)            166320    \n",
      "_________________________________________________________________\n",
      "lstm_338 (LSTM)              (None, 1, 90)             97560     \n",
      "_________________________________________________________________\n",
      "lstm_339 (LSTM)              (None, 30)                14520     \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 278,431\n",
      "Trainable params: 278,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 79275 samples, validate on 19819 samples\n",
      "Epoch 1/10\n",
      "79275/79275 [==============================] - 68s 862us/step - loss: 0.7116 - accuracy: 0.5307 - val_loss: 0.6920 - val_accuracy: 0.5243\n",
      "Epoch 2/10\n",
      "79275/79275 [==============================] - 79s 1ms/step - loss: 0.6927 - accuracy: 0.5303 - val_loss: 0.6912 - val_accuracy: 0.5159\n",
      "Epoch 3/10\n",
      "79232/79275 [============================>.] - ETA: 0s - loss: 0.6921 - accuracy: 0.5298"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-810daf0ca1ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                                          verbose=0)\n\u001b[0m\u001b[0;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                     \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(np.array(X_train).shape)\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = np.array(X_train).reshape((np.array(X_train).shape[0], 1,np.array(X_train).shape[1]))\n",
    "test_X = np.array(X_test).reshape((np.array(X_test).shape[0], 1, np.array(X_test).shape[1]))\n",
    "\n",
    "Model = Sequential()\n",
    "Model.add(LSTM(180, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "Model.add(LSTM(units=90, return_sequences=True))\n",
    "#Model.add(BatchNormalization())\n",
    "Model.add(LSTM(units=30))\n",
    "Model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "Model.summary()\n",
    "\n",
    "Model.fit(train_X,np.array(binary_train), epochs=10, batch_size=32, shuffle = False, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54480/54480 [==============================] - 12s 214us/step\n",
      "Accuracy :  0.5144640207290649\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy : ', Model.evaluate(np.array(test_X),np.array(binary_test))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
